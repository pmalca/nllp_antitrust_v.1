{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'H:\\My Drive\\a20151428@pucp.edu.pe 2023-10-29 03 16\\2. AcadÃ©mic\\16. nlp_antitrust\\material\\corpus.csv')\n",
    "df = df.dropna()\n",
    "df = df.applymap(lambda x: x.replace('\\n', ' ') if isinstance(x, str) else x)\n",
    "df = df.drop(columns=['Page', 'Text','nature_of_case','fact_summary'])\n",
    "df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "from nltk import word_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def text_preprocess(text: str) -> str:\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in word_tokenize(text.lower()) if (token.isalpha() and (token not in stop_words or token == \"per\" or token == \"se\"))]\n",
    "\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"rol_preprocessed\"] = df.rule_of_law.apply(text_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
